{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import datetime \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def file_input(self,filename):\n",
    "        data = pd.read_csv(filename,engine='python',header=None)\n",
    "        return data\n",
    "    \n",
    "    def my_function(self,list1,list2,list3):\n",
    "        if list3 == 1:\n",
    "            if list1 in list2:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    def convertCategory(self,x):\n",
    "        if x == 'S':\n",
    "            return -1\n",
    "        elif x in ['1','2','3','4','5','6','7','8','9','10','11','12']:\n",
    "            return x\n",
    "        else:\n",
    "            return 13\n",
    "    \n",
    "    def data_exploration_clicks(self,clicks):\n",
    "        \n",
    "        category_analaysis = clicks[(clicks['category'].isin(['S','0','1','2','3','4','5','6','7','8','9','10','11','12']))]\n",
    "      \n",
    "        chart_4 = sns.barplot(x=category_analaysis['category'].value_counts().index, y=category_analaysis['category'].value_counts())\n",
    "        chart_4.set_xticklabels(chart_4.get_xticklabels(),rotation=45)\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Count of clicks against each Category')\n",
    "        plt.show()\n",
    "   \n",
    "        chart_5 = sns.barplot(x=clicks['item_id'].value_counts().nlargest(10).index, y=clicks['item_id'].value_counts().nlargest(10))\n",
    "        chart_5.set_xticklabels(chart_5.get_xticklabels(),rotation=45)\n",
    "        plt.xlabel('Item_id')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Top 10 Items having maximum clicks:')\n",
    "        plt.show()\n",
    "        \n",
    "    def data_exploration_buys(self,data):\n",
    "        \n",
    "        print(\"Top 10 Items which have been bought the maximum.\")\n",
    "        #print(buys['item_id'].value_counts().nlargest(10))\n",
    "        chart_1 = sns.barplot(x=buys['item_id'].value_counts().nlargest(10).index, y=buys['item_id'].value_counts().nlargest(10))\n",
    "        chart_1.set_xticklabels(chart_1.get_xticklabels(),rotation=45)\n",
    "        plt.xlabel('Item_id')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Top 10 Items which have been bought the maximum.')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Top 10 items which are purchased in larger quantities.\")\n",
    "        quantity_analysis = buys[['item_id','qty']].groupby('item_id').agg(total_quantity=pd.NamedAgg(column='qty',aggfunc=sum))\n",
    "        quant_analysis = quantity_analysis.sort_values('total_quantity',ascending=False).nlargest(10,columns='total_quantity')\n",
    "        chart_2 = sns.barplot(x = quant_analysis.index, y = quant_analysis['total_quantity'] ,data = quant_analysis)\n",
    "        chart_2.set_xticklabels(chart_2.get_xticklabels(),rotation=45)\n",
    "        plt.xlabel('Item_id')\n",
    "        plt.ylabel('Quantity')\n",
    "        plt.title('Top 10 items which are purchased in larger quantities.')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Top 10 items Identifying the items having the maximum price.\")\n",
    "        buys_plot = buys[['item_id','price']].drop_duplicates().sort_values('price',ascending=False).nlargest(10,columns='price')\n",
    "        chart_3 = sns.barplot(x = buys_plot['item_id'] , y = buys_plot['price'] ,data = buys_plot)\n",
    "        chart_3.set_xticklabels(chart_3.get_xticklabels(),rotation=45)\n",
    "        plt.xlabel('Item_id')\n",
    "        plt.ylabel('Price')\n",
    "        plt.title('Top 10 items having the maximum cost price.')\n",
    "        plt.show()\n",
    "        \n",
    "    def transforming_buys(self,buys):\n",
    "        print(\"Transforming the buys file ...!!!\")\n",
    "        grouped = buys.groupby(\"session\")\n",
    "        buys_g = pd.DataFrame(index=grouped.groups.keys())        \n",
    "        buys_g[\"Number_items_bought\"] = grouped.item_id.count()\n",
    "        buys_g[\"unique_items_bought\"] = grouped.item_id.unique()\n",
    "        buys_g[\"is_buy\"] = 1\n",
    "        buys_g.index.name = \"session\"\n",
    "        print(\"Transformation of the buys file completed...!!!\")\n",
    "        return buys_g\n",
    "    \n",
    "    def chunk_load_data(self,chunk):\n",
    "        return chunk\n",
    "        \n",
    "        \n",
    "    def transforming_clicks(self,clicks):\n",
    "        \n",
    "        clicks_new = clicks.groupby('session')['timestamp'].agg([min,max])\n",
    "\n",
    "        clicks_new['dwell_time'] = clicks_new['max'] - clicks_new['min'] #cal the dwell time of the session.\n",
    "        clicks_new['dwell_time_seconds'] = clicks_new['dwell_time'].dt.total_seconds() #converting dwell time into seconds\n",
    "        \n",
    "        clicks.loc[clicks['category'] == 'S',['category']] = -1\n",
    "\n",
    "        grouped = clicks.groupby('session')\n",
    "            \n",
    "        #print(\"Calculating the total clicks\")\n",
    "        clicks_new['total_clicks'] = grouped.item_id.count()\n",
    "        \n",
    "        #print(\"Calculating the day of week\")\n",
    "        clicks_new['dayofweek'] = clicks_new['min'].dt.dayofweek\n",
    "        \n",
    "        #print(\"Calculating the day of month\")\n",
    "        clicks_new['dayofmonth'] = clicks_new['min'].dt.day\n",
    "        \n",
    "        #print(\"Calculating hour of click\")\n",
    "        clicks_new['hourofclick'] = clicks_new['min'].dt.hour\n",
    "        \n",
    "        #print(\"Calculating time of click\")\n",
    "        b = [0,4,8,12,16,20,24]\n",
    "        l = ['Late Night', 'Early Morning','Morning','Noon','Evening','Night']\n",
    "        clicks_new['timeofday'] = pd.cut(clicks_new['hourofclick'], bins=b, labels=l, include_lowest=True)\n",
    "        \n",
    "        #print(\"Calculating clickrate\")\n",
    "        clicks_new[\"click_rate\"] = clicks_new[\"total_clicks\"] / clicks_new[\"dwell_time_seconds\"]\n",
    "        clicks_new.click_rate = clicks_new.click_rate.replace(np.inf, np.nan)\n",
    "        clicks_new.click_rate = clicks_new.click_rate.fillna(0)\n",
    "        \n",
    "        #print(\"** Transformed**\")\n",
    "        return clicks_new\n",
    "\n",
    "    def transforming_clicks2(self,clicks):\n",
    "\n",
    "        grouped = clicks.groupby('session').agg({'item_id':['first','last','nunique'],'category':['nunique']})        \n",
    "        return grouped\n",
    "\n",
    "    def transforming_clicks3(self,clicks):\n",
    "       \n",
    "        keys, values = clicks.sort_values('session').values.T\n",
    "        ukeys, index = np.unique(keys, True)\n",
    "        arrays = np.split(values, index[1:])\n",
    "        df2 = pd.DataFrame({'a':ukeys, 'b':(a for a in arrays)})\n",
    "        return df2\n",
    "    \n",
    "    def transforming_clicks3_cat(self,clicks):\n",
    "       \n",
    "        keys, values = clicks.sort_values('session').values.T\n",
    "        ukeys, index = np.unique(keys, True)\n",
    "        arrays = np.split(values, index[1:])\n",
    "        df2 = pd.DataFrame({'a':ukeys, 'b':(a for a in arrays)})\n",
    "        return df2\n",
    "\n",
    "    def data_preparation(self,X,y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "        \n",
    "    def undersampling(self,train_data):   \n",
    "        \n",
    "        count_class_0, count_class_1 = train_data['is_buy'].value_counts()\n",
    "        df_class_0 = train_data[train_data['is_buy'] == 0]\n",
    "        df_class_1 = train_data[train_data['is_buy'] == 1]\n",
    "        df_class_0_under = df_class_0.sample(count_class_1)\n",
    "        df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "        df_test_under['is_buy'].value_counts()\n",
    "        return df_test_under\n",
    "        \n",
    "    def one_hot_encode(self,column_name,training_data):\n",
    "        temp = pd.get_dummies(training_data[column_name])\n",
    "        training_data = pd.concat([training_data, temp], axis=1)\n",
    "        return training_data\n",
    "        \n",
    "    def get_preds(self,threshold, probabilities):\n",
    "        return [1 if prob > threshold else 0 for prob in probabilities]\n",
    "        \n",
    "    def logit_model(self,train_x,train_y,test_x,test_y,thres=0.5):\n",
    "        \n",
    "        model = LogisticRegression(solver='sag')\n",
    "        model.fit(train_x,train_y.values.ravel())\n",
    "        probas = model.predict_proba(test_x)[:, 1]\n",
    "        print(\"Threshold Value : \",thres)\n",
    "        y_pred = self.get_preds(thres,probas)\n",
    "        return y_pred,probas\n",
    "\n",
    "    def calc_special_offer(self,x):\n",
    "        if -1 in x:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def error_metrics(self,prediction,test_y,probas):\n",
    "        \n",
    "        accuracy = accuracy_score(prediction,test_y)        \n",
    "        print('Accuracy =',accuracy)\n",
    "        print(\"\")\n",
    "        print(pd.DataFrame(confusion_matrix(test_y, prediction), columns=['Predicted 0', \"Predicted 1\"], index=['Actual 0', 'Actual 1']))\n",
    "        print(\"classification_Report:\")\n",
    "        print(classification_report(test_y,prediction))\n",
    "        fig, ax = plt.subplots(figsize=(10,7))\n",
    "        fpr, tpr, threshold = metrics.roc_curve(test_y,probas,pos_label=1)\n",
    "        i = np.arange(len(tpr)) \n",
    "        roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "        roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "        print(\"Optimum Threshold Value:\",list(roc_t['threshold']))\n",
    "        \n",
    "        auc = metrics.roc_auc_score(test_y, probas)\n",
    "        plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "        ax.plot(np.linspace(0, 1, 100),np.linspace(0, 1, 100),label='baseline',linestyle='--')\n",
    "        plt.title('Receiver Operating Characteristic Curve', fontsize=18)\n",
    "        plt.ylabel('TPR', fontsize=16)\n",
    "        plt.xlabel('FPR', fontsize=16)\n",
    "        plt.legend(fontsize=12);\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.ipynb train.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_file = \"../../data/yoochoose-clicks.dat\"\n",
    "buys_file = \"../../data/yoochoose-buys.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Chunk  1 /67\n",
      "Executing Chunk  2 /67\n",
      "Executing Chunk  3 /67\n",
      "Executing Chunk  4 /67\n",
      "Executing Chunk  5 /67\n",
      "Executing Chunk  6 /67\n",
      "Executing Chunk  7 /67\n",
      "Executing Chunk  8 /67\n",
      "Executing Chunk  9 /67\n",
      "Executing Chunk  10 /67\n",
      "Executing Chunk  11 /67\n",
      "Executing Chunk  12 /67\n",
      "Executing Chunk  13 /67\n",
      "Executing Chunk  14 /67\n",
      "Executing Chunk  15 /67\n",
      "Executing Chunk  16 /67\n",
      "Executing Chunk  17 /67\n",
      "Executing Chunk  18 /67\n",
      "Executing Chunk  19 /67\n",
      "Executing Chunk  20 /67\n",
      "Executing Chunk  21 /67\n",
      "Executing Chunk  22 /67\n",
      "Executing Chunk  23 /67\n",
      "Executing Chunk  24 /67\n",
      "Executing Chunk  25 /67\n",
      "Executing Chunk  26 /67\n",
      "Executing Chunk  27 /67\n",
      "Executing Chunk  28 /67\n",
      "Executing Chunk  29 /67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratikkasle/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Chunk  30 /67\n",
      "Executing Chunk  31 /67\n",
      "Executing Chunk  32 /67\n",
      "Executing Chunk  33 /67\n",
      "Executing Chunk  34 /67\n",
      "Executing Chunk  35 /67\n",
      "Executing Chunk  36 /67\n",
      "Executing Chunk  37 /67\n",
      "Executing Chunk  38 /67\n",
      "Executing Chunk  39 /67\n",
      "Executing Chunk  40 /67\n",
      "Executing Chunk  41 /67\n",
      "Executing Chunk  42 /67\n",
      "Executing Chunk  43 /67\n",
      "Executing Chunk  44 /67\n",
      "Executing Chunk  45 /67\n",
      "Executing Chunk  46 /67\n",
      "Executing Chunk  47 /67\n",
      "Executing Chunk  48 /67\n",
      "Executing Chunk  49 /67\n",
      "Executing Chunk  50 /67\n",
      "Executing Chunk  51 /67\n",
      "Executing Chunk  52 /67\n",
      "Executing Chunk  53 /67\n",
      "Executing Chunk  54 /67\n",
      "Executing Chunk  55 /67\n",
      "Executing Chunk  56 /67\n",
      "Executing Chunk  57 /67\n",
      "Executing Chunk  58 /67\n",
      "Executing Chunk  59 /67\n",
      "Executing Chunk  60 /67\n",
      "Executing Chunk  61 /67\n",
      "Executing Chunk  62 /67\n",
      "Executing Chunk  63 /67\n",
      "Executing Chunk  64 /67\n",
      "Executing Chunk  65 /67\n",
      "Executing Chunk  66 /67\n",
      "Executing Chunk  67 /67\n",
      "Done 1st Transformation of Clicks file to calculate \n",
      "1.Session Start Time \n",
      "2.Session End Time  \n",
      "3.Session Dwell Time \n",
      "4.Dwell time seconds \n",
      "5.Total clicks \n",
      "6.Dayofweek \n",
      "7.Dayofmonth \n",
      "8.Hourofclick \n",
      "9.Timeofday \n",
      "10.Click_Rate\n"
     ]
    }
   ],
   "source": [
    "data = Classifier()\n",
    "result=None\n",
    "count=1\n",
    "names=[\"session\", \"timestamp\", \"item_id\", \"category\"]\n",
    "for chunk in pd.read_csv(clicks_file,names=names,usecols = ['session','timestamp','item_id','category'],parse_dates=[\"timestamp\"],chunksize=500000):\n",
    "    print(\"Executing Chunk \",count,\"/67\")\n",
    "    click_df = data.transforming_clicks(chunk)\n",
    "    if result is None:\n",
    "      result=click_df\n",
    "      count=count+1\n",
    "    else:\n",
    "      result = result.append(click_df)  \n",
    "      count=count+1\n",
    "print(\"Done 1st Transformation of Clicks file to calculate \\n1.Session Start Time \\n2.Session End Time  \\n3.Session Dwell Time \\n4.Dwell time seconds \\n5.Total clicks \\n6.Dayofweek \\n7.Dayofmonth \\n8.Hourofclick \\n9.Timeofday \\n10.Click_Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Chunk  1 /67\n",
      "Executing Chunk  2 /67\n",
      "Executing Chunk  3 /67\n",
      "Executing Chunk  4 /67\n",
      "Executing Chunk  5 /67\n",
      "Executing Chunk  6 /67\n",
      "Executing Chunk  7 /67\n",
      "Executing Chunk  8 /67\n",
      "Executing Chunk  9 /67\n",
      "Executing Chunk  10 /67\n",
      "Executing Chunk  11 /67\n",
      "Executing Chunk  12 /67\n",
      "Executing Chunk  13 /67\n",
      "Executing Chunk  14 /67\n",
      "Executing Chunk  15 /67\n",
      "Executing Chunk  16 /67\n",
      "Executing Chunk  17 /67\n",
      "Executing Chunk  18 /67\n",
      "Executing Chunk  19 /67\n",
      "Executing Chunk  20 /67\n",
      "Executing Chunk  21 /67\n",
      "Executing Chunk  22 /67\n",
      "Executing Chunk  23 /67\n",
      "Executing Chunk  24 /67\n",
      "Executing Chunk  25 /67\n",
      "Executing Chunk  26 /67\n",
      "Executing Chunk  27 /67\n",
      "Executing Chunk  28 /67\n",
      "Executing Chunk  29 /67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratikkasle/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Chunk  30 /67\n",
      "Executing Chunk  31 /67\n",
      "Executing Chunk  32 /67\n",
      "Executing Chunk  33 /67\n",
      "Executing Chunk  34 /67\n",
      "Executing Chunk  35 /67\n",
      "Executing Chunk  36 /67\n",
      "Executing Chunk  37 /67\n",
      "Executing Chunk  38 /67\n",
      "Executing Chunk  39 /67\n",
      "Executing Chunk  40 /67\n",
      "Executing Chunk  41 /67\n",
      "Executing Chunk  42 /67\n",
      "Executing Chunk  43 /67\n",
      "Executing Chunk  44 /67\n",
      "Executing Chunk  45 /67\n",
      "Executing Chunk  46 /67\n",
      "Executing Chunk  47 /67\n",
      "Executing Chunk  48 /67\n",
      "Executing Chunk  49 /67\n",
      "Executing Chunk  50 /67\n",
      "Executing Chunk  51 /67\n",
      "Executing Chunk  52 /67\n",
      "Executing Chunk  53 /67\n",
      "Executing Chunk  54 /67\n",
      "Executing Chunk  55 /67\n",
      "Executing Chunk  56 /67\n"
     ]
    }
   ],
   "source": [
    "result_2=None\n",
    "count=1\n",
    "names=[\"session\", \"timestamp\", \"item_id\", \"category\"]\n",
    "for chunk in pd.read_csv(clicks_file,usecols = ['session','item_id','category'],names=names,chunksize=500000):\n",
    "    print(\"Executing Chunk \",count,\"/67\")\n",
    "    click_df = data.transforming_clicks2(chunk)\n",
    "    if result_2 is None:\n",
    "      result_2=click_df\n",
    "      count=count + 1\n",
    "    else:\n",
    "      result_2 = result_2.append(click_df) \n",
    "      count= count + 1\n",
    "print(\"Done Transforming Clicks input file to calculate \\n 1.First Clicked item \\n 2.Last Clicked Item \\n 3.Total Unique Items  \\n 4.Total Unique Categories \")\n",
    "colnames=['first_clicked_item','last_clicked_item','total_unique_items','total_unique_categories']\n",
    "result_2.columns = colnames\n",
    "#result_2.set_index('session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3=None\n",
    "count=1\n",
    "names=[\"session\", \"timestamp\", \"item_id\", \"category\"]\n",
    "for chunk in pd.read_csv(clicks_file,names=names,usecols = ['session','item_id'],chunksize=500000):\n",
    "    print(\"Executing Chunk \",count,\"/67\")\n",
    "    click_df = data.transforming_clicks3(chunk)\n",
    "    if result_3 is None:\n",
    "      result_3=click_df\n",
    "      count = count + 1\n",
    "    else:\n",
    "      result_3 = result_3.append(click_df) \n",
    "      count = count + 1\n",
    "    \n",
    "print(\"Done Transforming Clicks\")\n",
    "colnames=['session','visited_items']\n",
    "result_3.columns = colnames\n",
    "result_3 = result_3.set_index('session')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_4=None\n",
    "count=1\n",
    "names=[\"session\", \"timestamp\", \"item_id\", \"category\"]\n",
    "for chunk in pd.read_csv(clicks_file,names=names,usecols = ['session','category'],converters={\"category\": data.convertCategory},chunksize=500000):\n",
    "    print(\"Executing Chunk \",count,\"/67\")\n",
    "    click_df = data.transforming_clicks3_cat(chunk)\n",
    "    if result_4 is None:\n",
    "      result_4 = click_df\n",
    "      count = count + 1\n",
    "    else:\n",
    "      result_4 = result_4.append(click_df)  \n",
    "      count = count + 1\n",
    "colnames=['session','visited_categories']\n",
    "result_4.columns = colnames\n",
    "result_4 = result_4.set_index('session')\n",
    "result_4['Number_clicked_visited_categories'] = result_4['visited_categories'].apply(lambda x : len(x))\n",
    "result_4['Special_offer_click']=result_4['visited_categories'].apply(data.calc_special_offer)\n",
    "print(\"Done Transforming Clicks to calculate \\n 1.Unique Visited categories \\n 2. Total Visited Categories \\n 3.Special offer click \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys = data.file_input(buys_file)\n",
    "names=[\"session\",\"timestamp\",\"item_id\",\"price\",\"qty\"]\n",
    "buys.columns = names\n",
    "data.data_exploration_buys(buys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys_g = data.transforming_buys(buys)\n",
    "clicks_tranformed_updated = pd.concat([result,result_2,result_3,result_4], axis=1)\n",
    "training_data = pd.merge(clicks_tranformed_updated,buys_g['is_buy'],how='left',left_index=True,right_index=True)\n",
    "training_data['is_buy'] = training_data['is_buy'].fillna(0)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the popularity index for first and last item clicked\n",
    "print(\"calculating the popularity index for first and last item clicked\")\n",
    "names=[\"session\", \"timestamp\", \"item_id\", \"category\"]\n",
    "result_items = pd.concat([ chunk.apply(pd.Series.value_counts) for chunk in pd.read_csv(clicks_file,names=names,usecols = ['item_id'],index_col=0,chunksize=500000)])\n",
    "df = pd.DataFrame(result_items.index.value_counts())\n",
    "df.index.name = \"item_id\"\n",
    "df.columns = ['count']\n",
    "val = df['count'].sum()\n",
    "df['popularity'] = df['count'].apply(lambda x : x / val )\n",
    "df['popularity'] = df['popularity'].round(5)\n",
    "print(\"Done..!!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
